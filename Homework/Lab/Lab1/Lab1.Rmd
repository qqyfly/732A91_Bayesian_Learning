
---
title: "Bayesian Learning Lab 1"
author: 
  - Qinyuan Qi(qinqi464)
  - Satya Sai Naga Jaya Koushik	Pilla (satpi345)
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
# ===================================================================
# Init Library
# ===================================================================
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(gridExtra)
```

# 1. Daniel Bernoul 

We have $y_{1},\dots,y_{n}|\theta \sim Bern(\theta)$, and have obtained a sample with s = 22 successes in n = 70 trials. A $Beta(\alpha_{0},\beta_{0})$ prior for $\theta$ and let $\alpha_{0} = \beta_{0} = 8$ were also given.

```{r 1_init,echo = FALSE}
#--------------------------------------------------------------------
# Init code for question 1
#--------------------------------------------------------------------
rm(list = ls())
```

## (a) Draw random numbers from the posterior $\theta|y = Beta(\alpha_{0} + s,\beta_{0} + f)$, and verify that the posterior mean and sd converge to the true mean and sd as the number of draws increases.

First of all, we need to calculate the true mean and sd of Beta distribution, the following are the formulas.

$$
E[\theta] = \frac{\alpha}{\alpha + \beta} = \frac{\alpha_{0}}{\alpha_{0} + s + \beta_{0} + f}
$$

$$
sd[\theta] = \sqrt{\frac{\alpha\beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}} = \sqrt{\frac{(\alpha_{0}+s)(\beta_{0}+f)}{(\alpha_{0}+s + \beta_{0}+f)^2(\alpha_{0} +s + \beta_{0} + f + 1)}}
$$

Then we draw 10000 random values and calculate the mean and sd of the posterior distribution.

We found that the mean and sd of the posterior distribution did not converge to the true mean and sd as the number of draws increased but within a close range near the true values.

```{r 1a_0}
#--------------------------------------------------------------------
# Code for question 1a
#--------------------------------------------------------------------
# given values
nDraws_1a <- 10000
alpha0 <- 8
beta0 <- 8
s <- 22
n <- 70
f <- n - s
alpha_new <- alpha0 + s
beta_new <- beta0 + f

# calculate the true mean and standard deviation
true_mean <- alpha_new/(alpha_new + beta_new)
true_var <- (alpha_new*beta_new) / ((alpha_new + beta_new)^2 *
            (alpha_new + beta_new +1))
true_sd <- sqrt(true_var)

set.seed(12345)

# init the vectors to store the means and sds
means <- c()
sds <- c()

# calculate means and sds
for (i in 1:nDraws_1a) {
  beta_draws <- rbeta(n = i, shape1 = alpha_new, shape2 = beta_new)
  means[i] <- mean(beta_draws)
  sds[i] <- sd(beta_draws)
}
```

```{r 1a_plot,echo = FALSE,warning=FALSE}
# plot
df_1a <- data.frame(x = 1:nDraws_1a,
                   true_mean = true_mean,
                   true_sd = true_sd)
# create plot of mean and sd to show the converges status
mean_plot <- ggplot(data = df_1a) + 
  geom_point(aes(x = x, y = means, colour = "Samples"),size=0.1) +
  geom_hline(aes(yintercept = true_mean, colour = "True mean")) + 
  ggtitle("mean converges to the true values") + xlab("nDraws") + ylab("y")
sd_plot <- ggplot(data = df_1a) + 
  geom_point(aes(x = x, y = sds, colour = "Samples"),size=0.1)+
  geom_hline(aes(yintercept = true_sd,colour = "True sd")) +
  ggtitle("sd converges to the true values") + xlab("nDraws") + ylab("y")
grid.arrange(mean_plot, sd_plot, nrow = 2)
```

## (b) Draw 10000 random values from the posterior to compute the posterior probability $Pr(\theta > 0.3|y)$ and compare with the exact value from the Beta posterior

```{r 1b}
#--------------------------------------------------------------------
# Code for question 1b 
#--------------------------------------------------------------------
set.seed(12345)

nDraws_1b <- 10000
sample_b <- rbeta(n = nDraws_1b, shape1 = alpha_new, shape2 = beta_new)
prob_sample_b <- length(sample_b[which(sample_b > 0.3)])/ nDraws_1b
exact_value_1b  <- 1 - pbeta(q = 0.3, shape1 = alpha_new, shape2 = beta_new)
```
Code as above, and we have the following values listed in the table. The expected value is the exact value of the posterior probability $Pr(\theta > 0.3|y)$, and the simulated value is the value we calculated from the 10000 random values. And the expected value is a little bigger than the simulated value. And the exact value is will be calculated as $Pr(\theta > 0.3|y) = 1 - Pr(\theta <= 0.3|y)$ .

| Expected value               | Simulated vaule             |
|----------------              |-----------------            |
| `r round(exact_value_1b,4)`  | `r round(prob_sample_b,4)`  |

## (c) Draw 10000 random values from the posterior of the odds $\frac{\theta}{1-\theta}$ and plot the posterior distribution $\theta$.

```{r 1c}
#--------------------------------------------------------------------
# Code for question 1c
#--------------------------------------------------------------------
set.seed(12345)
nDraws_1c <- 10000
sample_1c <- rbeta(n = nDraws_1c, shape1 = alpha_new, shape2 = beta_new)
odds <- sample_1c/(1-sample_1c)
```

```{r 1c_plot,echo = FALSE}
# plot
hist(odds, probability= TRUE, breaks = 100)
lines(density(odds), col = "red")
```


# 2. Log-normal distribution and the Gini coefficient

```{r 2_init,echo = FALSE}
#--------------------------------------------------------------------
# Init code for question 2
#--------------------------------------------------------------------
rm(list = ls())
```

We asked 8 randomly selected persons about their income(in thousand SEK) and got the following data. 

$$
(33, 24, 48, 32, 55, 74, 23, 17)
$$

We know the model for non-negative continuous variables is the log-normal distribution $log N(\mu, \sigma^2)$ which has the following density function.

$$
p(y | \mu,\sigma^2) = \frac{1}{y\sqrt{2\pi\sigma^2}}exp(-\frac{(log(y)-\mu)^2}{2\sigma^2})
$$

where $y > 0 , -\infty < \mu < \infty, \sigma^2 > 0$

And we also know that if $y \sim logN(\mu,\sigma^2)$ , $\mu = 3.6$, but $\sigma$ unkown but with a prior $p(\sigma^2) \sim 1/\sigma^2$.

The posterior for $\sigma^2$ is the $Inv - \chi^2(n,\tau^2)$ distribution where .

$$
\tau^2 = \frac{\sum^{n}_{i=1}(log(y_i)-\mu)^2}{n}
$$

## (a) Draw 10000 random values from the posterior of $\sigma^2$ by assuming $\mu=3.6$ and plot the posterior distribution

According to the formula of posterior for $\sigma^2$, we have the following code.

```{r 2_a}
#--------------------------------------------------------------------
# Code for question 2a: calc tau value
#--------------------------------------------------------------------
set.seed(12345)

# given values
observations <- c(33, 24, 48, 32, 55, 74, 23, 17)
n_observations <- length(observations)
nDraws_2a <- 10000
mu <- 3.6
nDraws_2b <- 10000

# calculate tau square value and sigma_square
tau_square <- sum((log(observations)-mu)^2) / n_observations
X <- rchisq(nDraws_2b, n_observations)
sigma_square <- n_observations * tau_square / X
```

```{r 2_a_plot,echo = FALSE}
# plot
hist(sigma_square, probability= TRUE, breaks = 100)
lines(density(sigma_square), col = "red")
```

## (b) Use the posterior draws in a) to compute the posterior distribution of the Gini coefficient G for the current data set

From the statement of the question, we know that the Gini coefficient $G = 2 \phi(\sigma / \sqrt{2}) - 1$, and $\phi$ is the CDF of the Normal distribution with mean zero and unit variance, so $\mu = 0, sd = 1$. we calculate $\phi(\sigma / \sqrt{2})$ first then calculate the$G = 2 \phi - 1$, then we plot Gini coefficient value G.

```{r 2_b}
#--------------------------------------------------------------------
# Code for question 2b: compute the posterior distribution of the Gini
# coefficient
#--------------------------------------------------------------------
phi <- pnorm(sqrt(sigma_square/2), mean = 0, sd = 1)
G <- 2 * phi - 1
```
```{r 2_b_plot,echo = FALSE}
# Plot
hist(G, probability= TRUE, breaks = 100)
lines(density(G), col = "red")
```

## (c) Use the posterior draws from b) to compute a 95% equal tail credible interval for G.

95% equal tail credible interval for G is the interval between 0.025 and 0.975 of the distribution of G. We use the quantile method to calculate the interval and plot the histogram of G. 
```{r 2_c}
#--------------------------------------------------------------------
# Code for question 2c: compute a 95% equal tail credible interval for G
#--------------------------------------------------------------------
credible_interval <- quantile(G, c(0.025, 0.975))
```
```{r 2_c_plot,echo = FALSE}
# plot
hist(G, probability = TRUE, breaks = 100)
lines(density(G), col = "red")
abline(v = credible_interval, col = "blue")
```

The 95% equal tail credible interval for G is: (`r credible_interval[1]`, `r credible_interval[2]`)


## (d) Use the posterior draws from b) to compute a 95% Highest Posterior DensityInterval (HPDI) for G. Compare the two intervals in (c) and (d)

First, we calculate the kernel density estimate of the posterior of G using the density function, then make a data frame using density values and x values. Then we order it by y using descending order, calculate the cumulative sum, and filter the x value using the 95% HPDI. Based on the HPDI, we plot the histogram of G and the 95% HPDI interval.

```{r 2_d}
#--------------------------------------------------------------------
# Code for question 2d: compute a 95% Highest Posterior
# DensityInterval (HPDI) for G
#--------------------------------------------------------------------
density_G <- density(G)

# find the HPDI
# create the density data frame
density_G_df <- data.frame("x" = density_G$x, "y" = density_G$y)
# order it by y using descending order
density_G_df <- density_G_df[order(density_G_df$y, decreasing = TRUE),]
# calculate the cumulative sum
density_G_df$cumsum <- cumsum(density_G_df$y)
# find the 95% HPDI
HPDI <- density_G_df$x[density_G_df$cumsum < 0.95 * sum(density_G_df$y)]
hdpi_low <- min(HPDI)
hdpi_high <- max(HPDI)
```

```{r 2_d_plot,echo = FALSE}
# plot
hist(G, probability= TRUE, breaks = 100)
lines(density(G), col = "red")
abline(v = credible_interval, col = "blue")
abline(v = hdpi_low, col = "green", lty = 2)
abline(v = hdpi_high, col = "green", lty = 2)
```

95% HPDI interval for G is (`r hdpi_low`, `r hdpi_high`) which is green lines in the plot. The 95% equal tail credible interval for G is: (`r credible_interval[1]`, `r credible_interval[2]`) which is blue lines in the plot. 


Comparing the two intervals and plots above, we find that equal tail credible intervals cut off 2.5% of samples from each tail, while HDPI cuts off the data points that have the lowest density and keeps 95% of the data points. Since this distribution is not symmetrical, the HPDI interval is a better way to calculate the 95% interval in this case.



# 3. Bayesian inference for the concentration parameter in the von Mises distribution

We have observed wind directions at a given location on ten different days as follows.

$$
(20,314,285,40,308,314,299,296,303,326)
$$

Then those 10 angle-based(north is 0 degree) data are converted to radians-based data as follows.

$$
(-2.79,2.33,1.83,-2.44,2.23,2.33,2.07,2.02,2.14,2.54)
$$

According to the statement of the question, those data points conditional on $(\mu,k)$ are independent observations from the von Mises distribution as follows.

$$
p(y|\mu,k) = \frac{exp[k \cdot cos(y-\mu)]}{2 \pi I_{0}(k)}, -\pi \leqq y \leqq \pi
$$

Where $I_{0}(k)$ is the modified Bessel function of the first kind of order 0. The parameter $\mu (-\pi \leqq \mu \leqq \pi)$ is the mean direction and $k > 0$ is the concentration parameter, and have $\mu=2.4$. 

We also have $k \sim Exponential(\lambda = 0.5)$ a priori, where $\lambda$ is the rate parameter of the exponential distribution.

```{r 3_init,echo = FALSE}
#--------------------------------------------------------------------
# Init code for question 3
#--------------------------------------------------------------------
rm(list = ls())
```
 
## (a) Derive the expression for what the posterior $p(k|y,\mu)$ is proportional to. Hence, derive the function f(k) such that $p(k|y,\mu) \varpropto f(k)$. Then, plot the posterior distribution of k for the wind direction data over a fine grid of k values.

We know that 

$$
Posterior \propto Likelihood \times Prior
$$

So we derive the Likelihood formula of von Mises distribution as follows.

$$
\prod^n_{i=1} \frac{exp[k \cdot cos(y_{i}-\mu)]}{2 \pi I_{0}(k)}
$$

$$
= (\frac{1}{2 \pi I_{0}(k)})^n exp[k \cdot \sum^{n}_{i=1}{cos(y_{i}-\mu)}]
$$

$$
= \frac{1}{(2 \pi)^n}\frac{1}{(I_{0}(k))^n} exp[k \cdot \sum^{n}_{i=1}{cos(y_{i}-\mu)}]
$$

Since $\frac{1}{(2 \pi)^n}$ is not related to k, we can ignore it.

We have $k \sim Exponential$ a priori, the PDF of Exponential distribution (when $\lambda = 0.5 >0$)is.

$$
f(x;\lambda) = \lambda e^{-\lambda x} 
$$

With $\lambda = 1/2$, we have the following prior.

$$
\lambda exp(-\lambda k) = \frac{1}{2} exp(-\frac{1}{2} k) 
$$

Since 1/2 is not related to k, we can ignore it.

So we have posterior as follows.

$$
\frac{1}{(I_{0}(k))^n} exp[k \cdot \sum^{n}_{i=1}{cos(y-\mu)}] \cdot exp(-\frac{1}{2} k)  
$$

$$
= \frac{1}{(I_{0}(k))^n} exp[k \cdot \sum^{n}_{i=1}{cos(y-\mu)} -\frac{1}{2} k]
$$

So we write the following code to calculate the posterior distribution of k.

```{r 3a_1}
#--------------------------------------------------------------------
# Code for question 3a: calc the posterior
#--------------------------------------------------------------------
# given values
degrees <- c(20, 314, 285, 40, 308, 314, 299, 296, 303, 326)
n_degrees <- length(degrees)
radians <- c(-2.79, 2.33, 1.83, -2.44, 2.23, 2.33, 2.07, 2.02, 2.14, 2.54)
mu_3a <- 2.4
#mu_sample <- mean(radians)
#sd_sample <- sd(radians)

#radians <- (radians-mu_sample)/sd_sample
#mu_3a <- (mu_3a-mu_sample)/sd_sample
n_radians <- length(radians)

# grid of k values
k <- seq(from = 0.1,to = 10, by = 0.01) # k>0

# calculate the posterior
result_3a <- exp(k * sum(cos(radians - mu_3a)) - k / 2) /
  besselI(x = k, nu = 0) ^ n_radians
k_max_index <- which.max(result_3a)
```
Then we plot the posterior distribution of k for the wind direction data over a fine grid of k values 0 to 10 with step 0.1.

```{r 3a_2,echo = FALSE}
#-----------------------------------------
# Code for question 3a: plot the posterior
#-----------------------------------------
# create plot data
df_3a <- data.frame("k" = k, "posterior" = result_3a)

plot_3a <- ggplot(data = df_3a, aes(x=k, y=result_3a)) +
  geom_line() +
  ylab("Posterior value") +
  xlab("k value") +
  ggtitle("Posterior distribution of k")

plot_3a + geom_vline(aes(xintercept = k[k_max_index]))
```

## (b) Find the (approximate) posterior mode of k from the information in a.

```{r 3b}
#-----------------------------------------
# Code for question 3b: posterior mode of k 
#-----------------------------------------
k_mode <- k[k_max_index]
```
According to the data in 3.b, we know that the k is ```r k_mode```.




---
title: "Bayesian Learning Lab 2"
author: 
  - Qinyuan Qi(qinqi464)
  - Satya Sai Naga Jaya Koushik	Pilla (satpi345)
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
# ===================================================================
# Init Library
# ===================================================================
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(gridExtra)
library(readxl)
library(LaplacesDemon)
library(MASS)
#library(reshape2)
library(matrixStats)
library(mvtnorm)
```

# 1. Linear and polynomial regression

```{r 1_init,echo = FALSE}
#--------------------------------------------------------------------
# Init code for question 1
#--------------------------------------------------------------------
rm(list = ls())
```

We have a dataset Linkoping2022.xlsx that contains daily average temperatures (in degrees Celcius) in Link√∂ping over the year 2022. 

The response variable is temp and covariate time, the original data is a date-time string, it also needs to be converted to a numeric value using the following formula.

$$
time = \frac{\text{the time of days since the beginning of the year}}{365}
$$

A Bayesian analysis of the following quadratic regression model will be performed as follows.

$$
temp = \beta_{0} + \beta_{0} time + \beta_{2}  time^{2} + \epsilon \overset{\text{iid}}{\sim} N(0, \sigma^{2})
$$



## 1.a. Use the conjugate prior for the linear regression model

According to the question, we have the following init value.
```{r 1a_0}
#--------------------------------------------------------------------
# Code for question 1a
#--------------------------------------------------------------------
# given values
mu_0 <- c(0, 100, -100)
omega_0 <- 0.01 * diag(3)
v_0 <- 1
sigma2_0 <- 0.1
days_in_year <- 365
first_day_of_year <- as.Date("2022-01-01")
```

We calculate the parameter time,time_square according to the formula mentioned in the question.

```{r 1a_1}
#--------------------------------------------------------------------
# load data and plot
#--------------------------------------------------------------------
# load the data
temperature_data <- read_xlsx(path="Linkoping2022.xlsx")
temperature_data$time <- as.numeric(as.Date(temperature_data$datetime) - first_day_of_year) /
                         days_in_year
temperature_data$time_square <- temperature_data$time ** 2 
```
Then plot the original data to visualize the relationship between time and temperature.

```{r 1a_2, echo=FALSE}
# plot points to visualize 
plot_1a <- ggplot(temperature_data, aes(x = time, y=temp)) +
    geom_point(aes(color = "red")) +
    ylab("Temperature") + xlab("Time") + 
    ggtitle("Time vs Temperature") +
    scale_color_identity(name = "Colour", 
                       breaks = c("red"),
                       labels = c("Data"),
                       guide = "legend") +
  theme(plot.title = element_text(hjust = 0.5)) + 
  theme(legend.position="bottom")

plot_1a
```

A linear regression model with the formula mentioned above was built and the summary of the model as follows.

```{r 1a_3}
#--------------------------------------------------------------------
# make a linear regression model
#--------------------------------------------------------------------
linRegModel <- lm(temp ~ time + time_square, data = temperature_data)
summary(linRegModel)
```

According to the conjugate prior for the linear regression in slide 5 of Lecture 5, we have the following formula to simulating draws from the joint prior distribution.

Joint prior for $\beta$ and $\sigma^2$ are as follows.

$$
\beta|\sigma^2 \sim N(\mu_0, \sigma^2 \Omega_0^{-1})
$$

$$
\sigma^2 \sim Inv-\chi^2(v_0, \sigma^2_0)
$$

The posterior are as follows.

$$
\beta|\sigma^2,y \sim N(\mu_n, \sigma^2 \Omega_n^{-1})
$$

$$
\sigma^2|y \sim Inv-\chi^2(v_n, \sigma^2_n)
$$

$$
\mu_n = (X'X + \Omega_0)^{-1}(X'X\hat{\beta} + \Omega_0\mu_0)
$$

$$
\Omega_n = X'X + \Omega_0
$$

$$
v_n = v_0 + n
$$

From slide 4 of Lecture 5, we have the $\hat{\beta}$ formula as follows.

$$
\hat{\beta} = (X'X)^{-1}X'y
$$

$$
s^2 = \frac{1}{n-k} (y-X\hat{\beta})'(y-X\hat{\beta})
$$

Based on the above formula, we can simulate draws from the joint prior distribution of $\beta_{0},\beta_{1},\beta_{2} \text{ and } \sigma^2$ as follows. We will simulate 50 draws this time.

```{r 1a_4, echo=FALSE}

#--------------------------------------------------------------------
# function to simulate draws from the joint prior distribution
#--------------------------------------------------------------------
# function to simulate draws from the joint prior distribution
joint_conjugate_prior <- function(nr_of_iterations, v_0, sigma2_0, mu_0, omega_0, days_in_year=365){  
  # data frame to save prior coef data
  prior <- data.frame(matrix(ncol = 3, nrow = nr_of_iterations)) # for every beta one column

  # data frame to save linear regression line
  regline <- data.frame(matrix(nrow = days_in_year, ncol = nr_of_iterations)) # every col for one day
  
  # joint conjugate prior
  for (i in 1:nr_of_iterations) {
    
    #  calculate var
    var <- LaplacesDemon::rinvchisq(1,v_0,sigma2_0)
    
    # calculate beta
    beta <- MASS::mvrnorm(1, mu_0, var*solve(omega_0))
    
    # set values
    prior[i,1:3] <- beta
    regline[,i] <- beta[1] + beta[2] * temperature_data$time + beta[3] * temperature_data$time_square
  }
  
  # add meaningful column and row names
  for (i in 1:nr_of_iterations) {
    colnames(regline)[i] <- paste("pred","", i)
    rownames(regline)[i] <- paste("day","",i)
  }
  
  # return the prior and regline
  return(list(p = prior, r = regline))
}
```

```{r 1a_5, echo=FALSE}

#--------------------------------------------------------------------
# draw from joint_conjugate_prior and plot
#--------------------------------------------------------------------
# set random seed
set.seed(123456)

# joint conjugate prior
N <- 50

result <- joint_conjugate_prior(N, v_0, sigma2_0, mu_0, omega_0)
prior <- result$p
regline <- result$r

colors <- sample(colors(), N)

plot_1a_lines <- plot_1a

plot_1a_lines +
  mapply(function(i, col) {
    geom_line(aes(x = time, y = regline[, i]), col = col)
  }, 1:N, colors)
```

From the graph above, we found that at least 2 curves are not fitting the data well, which reach 3000 Celcius degree and 1000 Celcius degree. 
So we change the parameter to make the prior more suitable for the data. We change parameters and simulate 50 draws from
new joint prior distribution and plot it. The following is the new parameters.

```{r 1a_6}
#--------------------------------------------------------------------
# draw from joint_conjugate_prior and plot(new parameters)
#--------------------------------------------------------------------
# new given parameters
mu_0 <- c(-10, 100, -100)
omega_0 <- 0.03 * diag(3)
v_0 <- 2
sigma2_0 <- 0.1
```

```{r 1a_7, echo=FALSE}
# set random seed
set.seed(123456)

# joint conjugate prior
N <- 50

result <- joint_conjugate_prior(N, v_0, sigma2_0, mu_0, omega_0)
prior <- result$p
regline <- result$r

colors <- sample(colors(), N)

plot_1a_lines <- plot_1a

plot_1a_lines +
  mapply(function(i, col) {
    geom_line(aes(x = time, y = regline[, i]), col = col)
  }, 1:N, colors)
```

## 1.b. Write a function that simulates draws from the joint posterior distribution of $\beta_{0},\beta_{1},\beta_{2} \text{ and } \sigma^2$

We write a function that simulates draws from the joint posterior distribution of $\beta_{0},\beta_{1},\beta_{2} \text{ and } \sigma^2$ first.

```{r 1b_0}
#--------------------------------------------------------------------
# function to simulate draws from the joint posterior distribution
#--------------------------------------------------------------------
# function to simulate draws from the joint posterior distribution
joint_conjugate_posterior <- function(nr_of_iterations, v_0, sigma2_0, mu_0, omega_0, days_in_year=365){  
  # data frame to save prior coef data
  prior <- data.frame(matrix(ncol = 3, nrow = nr_of_iterations)) # for every beta one column

  # data frame to save linear regression line
  regline <- data.frame(matrix(nrow = days_in_year, ncol = nr_of_iterations)) # every col for one day
  
  # joint conjugate prior
  for (i in 1:nr_of_iterations) {
    
    #  calculate var
    var <- LaplacesDemon::rinvchisq(1,v_0,sigma2_0)
    
    # calculate beta
    beta <- MASS::mvrnorm(1, mu_0, var*solve(omega_0))
    
    # set values
    prior[i,1:3] <- beta
    regline[,i] <- beta[1] + beta[2] * temperature_data$time + beta[3] * temperature_data$time_square
  }
  
  # add meaningful column and row names
  for (i in 1:nr_of_iterations) {
    colnames(regline)[i] <- paste("pred","", i)
    rownames(regline)[i] <- paste("day","",i)
  }
  
  # return the prior and regline
  return(list(p = prior, r = regline))
}
```

### 1.b.i Plot a histogram for each marginal posterior of the parameters

According to the formulas mentioned in 1.a, we can calculate the posterior distribution of the parameters $\mu_n$, $\Omega_n$, $v_n$.

```{r 1b_i_0}
#--------------------------------------------------------------------
# calculate the posterior distribution parameters
#--------------------------------------------------------------------
k <- 3
X <- cbind(1, temperature_data$time, temperature_data$time_square) 
y <- temperature_data$temp
n <- nrow(temperature_data)
beta_hat <- solve(t(X) %*% X) %*% t(X) %*% y
mu_n <- solve(t(X) %*% X + omega_0) %*% (t(X) %*% X %*% beta_hat + omega_0 %*% mu_0)
omega_n <- t(X) %*% X + omega_0
v_n <- v_0 + n
sigma2_n <- (v_0 * sigma2_0 + t(y) %*% y + t(mu_0) %*% omega_0 %*% 
            mu_0 - t(mu_n) %*% omega_n %*% mu_n) / v_n
```

Marginal posterior of $\beta$ is a t-distribution with n-k freedom.

$$
\beta | y \sim t_{n-k}[\mu_n , s^2(X'X)^{-1}]
$$

```{r 1b_i_1}
#--------------------------------------------------------------------
# simulate the beta parameter and draw
#--------------------------------------------------------------------

# simulate the beta parameter
data_hist <- as.data.frame(
  mvtnorm::rmvt(n = 1000, delta = mu_n, df = n-k,
                sigma = as.numeric(sigma2_n) * solve(t(X) %*% X)))

# simulate the sigma^2 parameter 
var_hist <- LaplacesDemon::rinvchisq(n = 1000, v_n, sigma2_n)
```

```{r 1b_i_2, echo=FALSE}
data_hist <- cbind(data_hist, var_hist)
cnames <- c("beta0", "beta1", "beta2", "sigma2")
colnames(data_hist) <- cnames

# plot the histogram of 4 parameters
plot_func <- function(cname){
  ggplot(data_hist, aes(x = !!sym(cname))) +
    geom_histogram(aes(y = after_stat(density)),
                   colour = "black",
                   fill   = "white",
                   bins   = 30) +
    geom_density(alpha = .2, fill = "blue") 
}
plot(arrangeGrob(grobs = lapply(cnames, plot_func)))

```

### 1.b.ii Make a plot, and overlay curves for the 90% equal tail posterior probability intervals of f(time), then comment

We calculate median of the $\beta$ and f(time) for every time first.
```{r 1b_ii_0}
#--------------------------------------------------------------------
# calculate the median of beta and f(time)
#--------------------------------------------------------------------
data_hist = data_hist[,1:3] 

beta_median = matrixStats::colMedians(as.matrix(data_hist))
pred.1b <- beta_median %*% t(X)
```

Then we calculate the equal tail posterior probability intervals of f(time) 
and plot them on the graph below.
```{r 1b_ii_2}
#--------------------------------------------------------------------
# calculate the 90% credible interval
#--------------------------------------------------------------------
preds <- as.matrix(data_hist) %*% t(X) # regression function
pred_interval <- data.frame(nrow = n, nrow = 2)
colnames(pred_interval) <- c("lower","upper")

for(i in 1:days_in_year){
  data_t <- preds[,i]
  pred_interval[i,] <- quantile(data_t, probs = c(0.05,0.95))
}
```

```{r 1b_ii_3, echo=FALSE}

# plot data 
data.1b <- cbind(temperature_data, t(pred.1b),pred_interval)

ggplot(data.1b) +
  geom_point(aes(x = time, y = temp,color = "red")) +
  geom_line(aes(x = time, y = t(pred.1b),color = "blue"),linewidth=1) +
  geom_line(aes(x = time, y = lower,color = "green")) +
  geom_line(aes(x = time, y = upper,color = "green")) + 
  ggtitle("Time vs Temperature with pred curve and pred interval") +    
  scale_color_identity(name = "Colour", 
                       breaks = c("red", "blue", "green"),
                       labels = c("Data", "Pred Curve", "Posterior Probability Interval"),
                       guide = "legend") +
  theme(plot.title = element_text(hjust = 0.5)) + 
  theme(legend.position="bottom")
```

A posterior probability interval(the space between 2 green lines above) is an interval where the posterior
probability of the parameter within that interval exceeds a specified threshold. It is an interval to evaluate
the robustness of the parameter. It does not and should not contain most of the data points in our case.


## 1.c. Use the simulated draws in (b) to simulate the posterior distribution of  $\widetilde{x}$

We calculate the highest prediction for every day from the data in 1.b. 
Then we add this curve to the plot above and get the following graph.
```{r 1c_0}
#--------------------------------------------------------------------
# calculate the highest prediction for every day
#--------------------------------------------------------------------

pred_highest <- c()
for (i in 1:days_in_year) {
  pred_highest[i] <- max(preds[,i])
}
```

```{r 1c_plot, echo=FALSE}
data.1c <- cbind(temperature_data, t(pred.1b), pred_interval, pred_highest)
ggplot(data.1b) +
  geom_point(aes(x = time, y = temp,color = "red")) +
  geom_line(aes(x = time, y = t(pred.1b),color = "blue"),linewidth=1) +
  geom_line(aes(x = time, y = lower,color = "green")) +
  geom_line(aes(x = time, y = upper,color = "green")) + 
  geom_line(aes(x = time, y = pred_highest,color = "skyblue")) + 
  ggtitle("Time vs Temperature with pred curve, pred_interval and pred_highest") +
  scale_color_identity(name = "Colour", 
                       breaks = c("red", "blue", "green", "skyblue"),
                       labels = c("Data", "Pred Curve", "Posterior Probability Interval", "Pred Highest"),
                       guide = "legend") +
  theme(plot.title = element_text(hjust = 0.5)) + 
  theme(legend.position="bottom")
```

## 1.d. suitable prior that mitigates this potential problem

From slide 9 of Lecture 5, we know that too many knots lead to overfitting. 
We can use regularization prior to avoid the problem mentioned in the question.
A suitable prior that mitigates this potential problem is as follows.

$$
\beta_i \mid \sigma^2 \overset{\text{iid}}{\sim} N(\mu_0,\frac{\sigma^2}{\lambda}) \ \  where \ \ \Omega_0 = \lambda\cdot I
$$


# 2. Posterior approximation for classification with logistic regression

```{r 2_setup,echo=FALSE}
# clean up environment
rm(list=ls())
```

We have a dataset WomenAtWrok.dat that contains n=132 observations, and the 8 variables related to women are listed in the following table.

| Variable    | Data Type | Meaning                                     |  Role     |
|:-----------:|:---------:|:-------------------------------------------:|:---------:|
| Work        | Binary    | Whether or not the woman works              | Response y|
| Constant    | 1         | Constant to the intercept                   | Feature   |
| HusbandInc  | Numeric   | Husband's income                            | Feature   |
| EducYears   | Counts    | Years of education                          | Feature   |
| ExpYears    | Counts    | Years of experience                         | Feature   |
| Age         | Counts    | Age                                         | Feature   |
| NSmallChild | Counts    | Number of child $\leq$ 6 years in household | Feature   |
| NBigChild   | Counts    | Number of child > 6 years in household      | Feature   |

## 2.a. Approximate the posterior distribution of the parameter vector $\beta$ with a multivariate normal distribution and comment. compute 95% equal tail posterior probability interval and comment.

We have the following logistic regression model, if y=1 means woman works and 0 means woman not work.

$$
Pr(y=1|x,\beta) = \frac{exp(x^T\beta)}{1+exp(x^T\beta)}
$$

Posterior distribution of the parameter vectoor $\beta$ with a multivariate normal distribution as follows.

$$
\beta|y,x \sim N(\tilde{\beta}, J^{-1}_{y}(\tilde{\beta}))
$$

where $\tilde{\beta}$ is the posterior mode and
$J(\tilde{\beta}) = - \frac{\partial^2 ln p(\beta|y)}{\partial\beta\partial\beta^T}|_{\beta = \tilde{\beta}}$
is the negative of the observed Hessian evaluated at the posterior mode.

Also we use prior $\beta \sim N(\mu, \tau^{2}I)$, where $\mu=0$ and $\tau = 2$, so we have the following init values.

```{r 2a_0}
#--------------------------------------------------------------------
# given values 
#--------------------------------------------------------------------
mu <- 0
tau <- 2
```

To use optim function, we create the following function LogPostLogistic, then we can get the optimized 
$\tilde{\beta}$ and $J^{-1}_{y}(\tilde{\beta})$

```{r 2a_5,echo=FALSE}
#--------------------------------------------------------------------
# load the data
#--------------------------------------------------------------------
WomenAtWork = read.table("WomenAtWork.dat", header = TRUE)
y = WomenAtWork$Work
X = WomenAtWork[,2:8]
X = as.matrix(X)
# Xnames will be used as the names of the output of beta and sigma
Xnames <- colnames(X)
```

```{r 2a_1}
#--------------------------------------------------------------------
# Functions that returns the log posterior for the logistic
#--------------------------------------------------------------------

LogPostLogistic <- function(betas,y,X,mu,sigma){
  linPred <- X%*%betas;
  logLik <- sum( linPred*y - log(1 + exp(linPred)) );  
  # Likelihood is not finite, stear the optimizer away from here!
  # Idea from cource code by teacher
  if (abs(logLik) == Inf)
    logLik = -20000;  
  # prior follows multi-normal distribution
  logPrior <- dmvnorm(betas, mu, sigma, log=TRUE);
  return(logLik + logPrior)
}

#--------------------------------------------------------------------
# get the optimized beta and inverse jacobian
#--------------------------------------------------------------------

# number of features
n <- dim(X)[2]
# setting up the prior
mu    <- as.vector(rep(mu,n))  # Prior mean vector
sigma <- tau^2 * diag(n)  # Prior variance matrix

# use random initial values
init_val <- as.vector(rnorm(dim(X)[2]))
# optimize the log posterior
OptimRes <- optim(init_val,
                  LogPostLogistic,
                  y = y,
                  X = X,
                  mu = mu,
                  sigma = sigma,
                  method=c("BFGS"),
                  control=list(fnscale=-1),
                  hessian=TRUE)
# set values to print out
posterior_mode  <- OptimRes$par

# hessian is the negative of the observed Hessian evaluated at the posterior mode
# Jacobian = (-hessian)
beta_jacobian <- -OptimRes$hessian
beta_inverse_jacobian <- solve(beta_jacobian) 
```

```{r 2a_2, echo=FALSE}
# Naming the coefficient by Xnames
names(posterior_mode) <- Xnames 
# Naming the coefficient by Xnames
colnames(beta_inverse_jacobian) <- colnames(X) 

print('The posterior beta is:')
print(posterior_mode)
print('The Inverse Jacobian of beta is:')
print(beta_inverse_jacobian)
```
We can find that NSmallChild has the biggest influence on the work value, which is -1.47239519. 
From the beta values, we find that the number corresponding to the variable NSmallChild is 
also a negative number, which means the more NSmallChild, the less likely to work.

Then we compute an approximate 95% equal tail posterior probability interval for the 
regression coefficient to the variable NSmallChild.We find that the whole interval is
negative, which means NSmallChild has an obvious negative impact on women's work or not.

```{r 2a_3}
#--------------------------------------------------------------------
# Compute an approximate 95% equal tail posterior probability interval
#--------------------------------------------------------------------
beta_sim <- mvtnorm::rmvnorm(n = 1000, mean = posterior_mode, sigma = beta_inverse_jacobian)
data.NSmallChild <- beta_sim[,6]

# calculate the 95% credible interval
pred_interval <- quantile(data.NSmallChild, probs = c(0.025,0.975))
pred_interval
```
We also use glm to genertate logistic regression model.

```{r 2a_6}
#--------------------------------------------------------------------
# logistic regression
#--------------------------------------------------------------------
glmModel <- glm(Work ~ 0 + ., data = WomenAtWork, family = binomial)
summary(glmModel)
```
Compare with the result above, we can find that the posterior result is similar to
the result of logistic regression generated by glm.

## 2.b. Write a function that simulates draws from the posterior predictive distribution and plot

According to the question, and the formula provided in 2.a 

$$
Pr(y=1|x,\beta) = \frac{exp(x^T\beta)}{1+exp(x^T\beta)}
$$

So we know that
$$
Pr(y=0|x,\beta) = 1 - Pr(y=1|x,\beta)
$$

Then we write the following function.

```{r 2b_0}
#--------------------------------------------------------------------
# Simulates draws from the posterior predictive distribution
#--------------------------------------------------------------------
post_pred <- function(X, beta) {  
  pred <- beta %*% X #linear predictions
  pred_logit <- 1- exp(pred)/(1 + exp(pred)) #sigmoid function to model probabilities
  return(data.frame(Probability = pred_logit))
}
```

we have the following init value, and we can get the posterior predictive distribution and plot.

```{r 2b_1}
#Create matrix of sample features
X_data <- as.matrix(c(0, 18, 11, 7, 40, 1, 1))
#function call on sample data
post_pred_sim <- post_pred(X_data, beta_sim)
```

```{r 2b_plot1, echo=FALSE}
#plotting the posterior prediction density & histogram
ggplot(data = post_pred_sim, aes(x = Probability)) + xlim(c(0,1)) +
  geom_histogram(aes(y = after_stat(density)), 
                 bins = 100, 
                 color = "black", 
                 fill = "grey") +
  geom_density(alpha = 0.2, fill = "blue") + 
  geom_vline(xintercept = 0.5, alpha = 0.8,  color = "red")

```

From the density of the posterior prediction, we can find that given the life conditions above , women have more probability to choose not to work.

## 2.c. Rewrite your function and plot

We have 13 women have the same features as in 2b, we rewrite the function and plot the result as below.

```{r 2c_0}
#--------------------------------------------------------------------
# Get the trial number and plot
#--------------------------------------------------------------------
n <- 13
post_pred_binom <- function(X, beta) {
  
  pred <- beta %*% X #linear predictions
  pred_logit <- 1- exp(pred)/(1 + exp(pred)) #sigmoid function to model probabilities  
  trial <- c()
  for (i in 1:length(pred_logit)) {
    trial[i] <- rbinom(n = 1, size = n, prob = pred_logit[i])
  }  
  return(trial)
}

test_trials <- post_pred_binom(X_data, beta_sim)
hist(test_trials)
```

According to the plot above, histogram of test_trials match the result of 2b, and around 10-12 out of 13 are likelily not working given the given features.